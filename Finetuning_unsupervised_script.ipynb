{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef608954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Random Seed:  9432\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "\n",
    "from model_architecture import *\n",
    "from losses_paired import *  # we use the ranking loss but only on intra-subject pairs + consistency loss\n",
    "from Dataloader.Dataloader import *\n",
    "\n",
    "\n",
    "\n",
    "######################## configure device ###############\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# verify that gpu is recognized\n",
    "print(device)\n",
    "\n",
    "################## Set random seem for reproducibility ##########\n",
    "manualSeed = 9432\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "########## interactive mode for plots ###################\n",
    "plt.ion()   \n",
    "%matplotlib inline\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f581bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_inference(val_loader):\n",
    "    encoder_model.eval()\n",
    "    classifier_model.eval()\n",
    "    \n",
    "    t_inp=torch.from_numpy(np.array([6.0, 12.0, 18.0, 24.0, 30.0, 36.0])).to(device)\n",
    "    t_inp=(t_inp/36.0)\n",
    "    \n",
    "    gt_lst=[] # 2D array:  sample, 6 time-points\n",
    "    pred_lst=[]\n",
    "    risk_scr_lst=[]\n",
    "    tcnv_lst=[]\n",
    "    indctr_lst=[]\n",
    "    nm_lst=[]\n",
    "    \n",
    "    for i, sample in enumerate(val_loader):\n",
    "        img=sample['img'].to(device)\n",
    "        gt=sample['gt'] # 6-D  conversion within 0/6/12/18/24/30/36 month time-points\n",
    "        tcnv=sample['tcnv']\n",
    "        indctr=sample['indctr']\n",
    "        nm=sample['nm']\n",
    "        \n",
    "        img=rearrange(img, 'b 1 c h w -> b c h w')\n",
    "        \n",
    "        ### Forward Pass\n",
    "        with torch.no_grad():\n",
    "            ftr=encoder_model(img)                    # 2B,768\n",
    "            \n",
    "            tmp=t_inp.unsqueeze(dim=0).repeat(img.shape[0],1).to(dtype=torch.float32)\n",
    "            rsk, pred_logits=classifier_model(ftr, tmp) # 3B, 1  and 3B,1\n",
    "            pred=F.sigmoid(pred_logits)\n",
    "            del tmp\n",
    "        \n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        ################################################\n",
    "        \n",
    "        pred_lst.append(pred)\n",
    "        risk_scr_lst.append(rsk.detach().cpu().numpy()) # risk predicted for the current input scan\n",
    "        nm_lst.append(nm)\n",
    "        gt_lst.append(gt)\n",
    "        tcnv_lst.append(tcnv)\n",
    "        indctr_lst.append(indctr)\n",
    "        \n",
    "        del img, gt, tcnv, indctr, nm,rsk, pred\n",
    "    \n",
    "    encoder_model.train()\n",
    "    classifier_model.train()\n",
    "    \n",
    "    \n",
    "    pred_lst=np.concatenate(pred_lst, axis=0) # or stack?  # B,6\n",
    "    risk_scr_lst=np.concatenate(risk_scr_lst, axis=0)      # B,1\n",
    "    nm_lst=np.concatenate(nm_lst, axis=0)                  # B,\n",
    "    gt_lst=np.concatenate(gt_lst, axis=0)                  # B,6\n",
    "    tcnv_lst=np.concatenate(tcnv_lst, axis=0)              # B,\n",
    "    indctr_lst=np.concatenate(indctr_lst, axis=0)          # B,\n",
    "    \n",
    "    ### Sort the list which is used to define the index for the samples of each bootstrap re-sampling.\n",
    "    idx=np.argsort(nm_lst, axis=0)\n",
    "    pred_lst=pred_lst[idx,:]           # B,6\n",
    "    risk_scr_lst=risk_scr_lst[idx,:]   # B,1\n",
    "    nm_lst=nm_lst[idx]                 # B,\n",
    "    gt_lst=gt_lst[idx,:]               # B,6\n",
    "    tcnv_lst=tcnv_lst[idx]             # B,\n",
    "    indctr_lst=indctr_lst[idx]         # B,\n",
    "    del idx\n",
    "    \n",
    "    ### Check that the sorted nm_lst is same as the validation dataloader\n",
    "    flag=np.array_equal(nm_lst, val_data.nm_lst) # this is the order used to define bootstrap samplings\n",
    "    if flag==False:\n",
    "        print('the nm_lst doesnot match !')\n",
    "    \n",
    "    \n",
    "    ###### Now everything is sorted by name. So now, we can use the pre-saved indices ###\n",
    "    indices=val_data.sampling_index\n",
    "    c_lst=[]\n",
    "    auc_lst=[]\n",
    "    for k in range(0, len(indices)): # No. of bootstrap re-samplings\n",
    "        idx=indices[k]\n",
    "        tmp_rsk=np.squeeze(risk_scr_lst[idx,:], axis=1) # B,\n",
    "        tmp_tcnv=tcnv_lst[idx]\n",
    "        tmp_indctr=indctr_lst[idx]\n",
    "        c_index = concordance_index_censored(tmp_indctr.astype(bool), tmp_tcnv, tmp_rsk)\n",
    "        c_index = c_index[0]\n",
    "        c_lst.append(c_index)\n",
    "        del c_index, tmp_indctr, tmp_tcnv, tmp_rsk\n",
    "        \n",
    "        tmp_gt=gt_lst[idx,:]\n",
    "        tmp_pred=pred_lst[idx,:]\n",
    "        auc=[]\n",
    "        for cls in range(0,6):\n",
    "            gt=tmp_gt[:, cls]\n",
    "            pred=tmp_pred[:, cls]\n",
    "            idx2=np.where(gt !=-1) # -1 implies GT is unavailable (eg. time after censoring has occured)\n",
    "            gt=gt[idx2]\n",
    "            pred=pred[idx2]\n",
    "            # roc_auc_score(y_true, y_score\n",
    "            auc.append(roc_auc_score(gt, pred))\n",
    "            del gt, pred, idx2\n",
    "            # suppose within timepoint 36 months but image is censored at 24 months. then lbl is -1, needs to be avoided\n",
    "        \n",
    "        auc=np.expand_dims(np.array(auc), axis=0)\n",
    "        auc_lst.append(auc) # list of 6-dim arrays\n",
    "        del auc, idx\n",
    "        \n",
    "    \n",
    "    auc_lst=np.concatenate(auc_lst, axis=0) # B,6\n",
    "    c_lst=np.array(c_lst)\n",
    "    \n",
    "    mean_concordance=np.mean(c_lst)\n",
    "    avg_auc=np.mean(auc_lst, axis=1) # avg across 6 time-points\n",
    "    mn_avg_auc=np.mean(avg_auc, axis=0) # avg across each sampling.\n",
    "    # confidence intervals np.percentile(c_lst, 0.95)   and 0.05\n",
    "    \n",
    "    print('\\n CI: '+str(mean_concordance)+'  AUC: '+str(mn_avg_auc))\n",
    "    metric=mean_concordance+mn_avg_auc # this has to be maximized\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13907d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(sample, optimizer, scheduler):\n",
    "    \n",
    "    img1=sample['img1'].to(device)   # B,3,224,224\n",
    "    img2=sample['img2'].to(device)   # B,3,224,224\n",
    "    \n",
    "    img1=rearrange(img1, '1 b 1 c h w -> b c h w')\n",
    "    img2=rearrange(img2, '1 b 1 c h w -> b c h w')\n",
    "    \n",
    "    tintrvl=torch.unsqueeze(torch.squeeze(sample['tintrvl'], dim=0), dim=1).to(device)  # B,1\n",
    "    \n",
    "    \n",
    "    ########################  Forward Pass ########################\n",
    "    B=img1.shape[0]\n",
    "    img=torch.cat((img1, img2), dim=0)                  # 2B,3,H,W\n",
    "    \n",
    "    #### prediction for img1,img2,img1_to_ftr ####\n",
    "    ftr=encoder_model(img)                    # 2B,768\n",
    "    \n",
    "    # Now add \n",
    "    t=torch.cat((torch.zeros((2*B, 1)).to(device), tintrvl), dim=0) # 3B,1\n",
    "    # for first 2B time, evaluate current risk and predictions so t=0 for last B, predict risk from img1\n",
    "    ftr=torch.cat((ftr, ftr[0:B, :]), dim=0)\n",
    "    rsk, pred_logits=classifier_model(ftr, t) # 3B, 1  and 3B,1\n",
    "        \n",
    "    ####################################### COMPUTE LOSSES ########################################\n",
    "    # gt1(O:N, 1:0);  gt2(0:N/2, 1:N/2);   tot 0:1.5N, 1:.5N\n",
    "    # gt3(0:N/2, 1:N/2)   \n",
    "    \n",
    "    #### Consistency Loss ####\n",
    "    cnstncy_loss=F.binary_cross_entropy_with_logits(pred_logits[(2*B):, 0:1], F.sigmoid(pred_logits[B:(2*B), 0:1]))\n",
    "    \n",
    "    ###################### Risk score ranking Loss ###########\n",
    "    rank_loss=my_risk_concordance_loss_paired(rsk[0:B, 0:1], rsk[B:(2*B),0:1])\n",
    "                                       \n",
    "        \n",
    "    loss=cnstncy_loss+rank_loss\n",
    "    ################# Backpropagation ###########################\n",
    "    # remove previously stored gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Compute Gradients\n",
    "    loss.backward()\n",
    "    # Update weights\n",
    "    #optimizer.step([cls_loss, cnstncy_loss, rank_loss], [1, 1, 1], None)\n",
    "    optimizer.step()\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    ############  Return Loss for Displaying #####\n",
    "    #cls_loss=cls_loss.detach().cpu().numpy()\n",
    "    cnstncy_loss=cnstncy_loss.detach().cpu().numpy()\n",
    "    rank_loss=rank_loss.detach().cpu().numpy()\n",
    "    \n",
    "    loss=loss.detach().cpu().numpy()\n",
    "    \n",
    "    return loss, cnstncy_loss, rank_loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1114ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_complete():\n",
    "    mn_lr=10**(-6.0)\n",
    "    mx_lr=10**(-4.0)\n",
    "    \n",
    "    nupdates=200#1000 # no of batch updates in each epoch\n",
    "    max_epochs=200\n",
    "    max_patience=100 # Early stopping if validation metric doesnot improve in this many consecutive epochs\n",
    "    \n",
    "    ####\n",
    "    max_metric=complete_inference(val_loader)\n",
    "    \n",
    "    ############################################################################################################\n",
    "    optimizer = torch.optim.AdamW(list(encoder_model.parameters()) + list(classifier_model.parameters()), lr=mn_lr, amsgrad=True)\n",
    "    scheduler=torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=mn_lr, max_lr=mx_lr, cycle_momentum=False,\n",
    "                                            step_size_up=nupdates//2, step_size_down=None, mode='triangular')\n",
    "    ############################################################################################################\n",
    "    \n",
    "    patience=0\n",
    "    data_iter = iter(train_loader) \n",
    "    for epochs in range(0, max_epochs):\n",
    "        run_loss=0 # total loss  \n",
    "        #run_cls_loss=0 #  BCE loss(mixup) with GT (both Encoder and ODE)\n",
    "        run_cnstncy_loss=0 # MSE loss of ODE feature pred\n",
    "        run_rnk_loss=0  # Concordance Index loss\n",
    "        \n",
    "        tic=time.time()\n",
    "        for i in range(0, nupdates): # batch updates in each round of training (epoch) \n",
    "            try:\n",
    "                sample = next(data_iter) \n",
    "            except StopIteration:\n",
    "                # StopIteration is thrown if dataset ends\n",
    "                # reinitialize data loader \n",
    "                data_iter = iter(train_loader)\n",
    "                sample = next(data_iter)\n",
    "                \n",
    "            \n",
    "            \n",
    "            loss, cnstncy_loss, rank_loss, optimizer, scheduler=train_one_batch(sample, optimizer, scheduler)\n",
    "            del sample\n",
    "            \n",
    "                \n",
    "            run_loss=run_loss+loss\n",
    "            #run_cls_loss=run_cls_loss+cls_loss\n",
    "            run_cnstncy_loss=run_cnstncy_loss+cnstncy_loss\n",
    "            run_rnk_loss=run_rnk_loss+rank_loss\n",
    "            \n",
    "            del loss,  cnstncy_loss, rank_loss\n",
    "                \n",
    "            if (i+1) % 10== 0: # displays after every 10 batch updates\n",
    "                print (\"Epoch [{}/{}], Batch [{}/{}], Train Loss: {:.4f}, CONSISTENCY: {:.4f}, RANKING: {:.4f}\"\n",
    "                       .format(epochs+1, max_epochs, i+1, nupdates, (run_loss/i), (run_cnstncy_loss/i), (run_rnk_loss/i)), end =\"\\r\")\n",
    "        \n",
    "            \n",
    "        ### End of an epoch. Check validation loss\n",
    "        metric=complete_inference(val_loader) \n",
    "        toc=time.time()\n",
    "        print('\\n Val Metric: '+str(metric)+'  Last Epoch took '+str(toc-tic)+' seconds')\n",
    "        \n",
    "        \n",
    "        run_loss=0 # total loss  \n",
    "        #run_cls_loss=0 #  BCE loss(mixup) with GT (both Encoder and ODE)\n",
    "        run_cnstncy_loss=0  # BCE loss of consistency in pred for ODE\n",
    "        run_rnk_loss=0  # Concordance Index loss\n",
    "        \n",
    "        #### Early stopping\n",
    "        if metric>max_metric:\n",
    "            max_metric=metric\n",
    "            patience=0\n",
    "            print('Validation metric improved !')\n",
    "            torch.save({\n",
    "                        'encoder_state_dict': encoder_model.state_dict(),\n",
    "                        'classifier_state_dict': classifier_model.state_dict()\n",
    "            },'best_weight_fld'+str(fld)+'_metric'+str(metric)+'.pt')\n",
    "        else:\n",
    "            patience=patience+1\n",
    "            print('\\n Validation metric has not improved in last '+str(patience)+' epochs')\n",
    "            if patience>max_patience:\n",
    "                print('Early Stopping !')\n",
    "                break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728f4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4bd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss_logits=nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([3.0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62273bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903\n",
      "\n",
      " CI: 0.6558512329643176  AUC: 0.6601325265482484\n",
      "Epoch [1/200], Batch [200/200], Train Loss: 0.7310, CONSISTENCY: 0.0417, RANKING: 0.6894\n",
      " CI: 0.7287608495785871  AUC: 0.7342547838134881\n",
      "\n",
      " Val Metric: 1.4630156333920752  Last Epoch took 180.71966552734375 seconds\n",
      "Validation metric improved !\n",
      "Epoch [2/200], Batch [200/200], Train Loss: 0.6412, CONSISTENCY: 0.0072, RANKING: 0.6340\n",
      " CI: 0.7301478960558329  AUC: 0.7377587326447499\n",
      "\n",
      " Val Metric: 1.4679066287005829  Last Epoch took 176.84829139709473 seconds\n",
      "Validation metric improved !\n",
      "Epoch [3/200], Batch [200/200], Train Loss: 0.6181, CONSISTENCY: 0.0030, RANKING: 0.6152\n",
      " CI: 0.7321734441306823  AUC: 0.7425580236948551\n",
      "\n",
      " Val Metric: 1.4747314678255374  Last Epoch took 179.7371346950531 seconds\n",
      "Validation metric improved !\n",
      "Epoch [4/200], Batch [200/200], Train Loss: 0.6151, CONSISTENCY: 0.0088, RANKING: 0.6063\n",
      " CI: 0.7236163458120576  AUC: 0.7218453475260985\n",
      "\n",
      " Val Metric: 1.445461693338156  Last Epoch took 178.49827075004578 seconds\n",
      "\n",
      " Validation metric has not improved in last 1 epochs\n",
      "Epoch [5/200], Batch [200/200], Train Loss: 0.5928, CONSISTENCY: 0.0010, RANKING: 0.5917\n",
      " CI: 0.7356687461325594  AUC: 0.7408533368760573\n",
      "\n",
      " Val Metric: 1.4765220830086165  Last Epoch took 177.22550344467163 seconds\n",
      "Validation metric improved !\n",
      "Epoch [6/200], Batch [200/200], Train Loss: 0.5980, CONSISTENCY: 0.0067, RANKING: 0.5914\n",
      " CI: 0.7396172070166938  AUC: 0.7476657818794776\n",
      "\n",
      " Val Metric: 1.4872829888961714  Last Epoch took 181.8619351387024 seconds\n",
      "Validation metric improved !\n",
      "Epoch [7/200], Batch [200/200], Train Loss: 0.5861, CONSISTENCY: 0.0025, RANKING: 0.5837\n",
      " CI: 0.7523750204670592  AUC: 0.7571742413930103\n",
      "\n",
      " Val Metric: 1.5095492618600694  Last Epoch took 178.58158707618713 seconds\n",
      "Validation metric improved !\n",
      "Epoch [8/200], Batch [200/200], Train Loss: 0.5797, CONSISTENCY: 0.0013, RANKING: 0.5784\n",
      " CI: 0.7296162991460217  AUC: 0.7323796577915804\n",
      "\n",
      " Val Metric: 1.461995956937602  Last Epoch took 178.32590413093567 seconds\n",
      "\n",
      " Validation metric has not improved in last 1 epochs\n",
      "Epoch [9/200], Batch [200/200], Train Loss: 0.5717, CONSISTENCY: 0.0003, RANKING: 0.5714\n",
      " CI: 0.6331388277009133  AUC: 0.6176749076557291\n",
      "\n",
      " Val Metric: 1.2508137353566424  Last Epoch took 179.12050557136536 seconds\n",
      "\n",
      " Validation metric has not improved in last 2 epochs\n",
      "Epoch [10/200], Batch [200/200], Train Loss: 0.5813, CONSISTENCY: 0.0007, RANKING: 0.5807\n",
      " CI: 0.7142151864896905  AUC: 0.7211555931436702\n",
      "\n",
      " Val Metric: 1.4353707796333608  Last Epoch took 177.2277774810791 seconds\n",
      "\n",
      " Validation metric has not improved in last 3 epochs\n",
      "Epoch [11/200], Batch [200/200], Train Loss: 0.5747, CONSISTENCY: 0.0017, RANKING: 0.5730\n",
      " CI: 0.7161283410380349  AUC: 0.7171234783640124\n",
      "\n",
      " Val Metric: 1.4332518194020474  Last Epoch took 181.36492943763733 seconds\n",
      "\n",
      " Validation metric has not improved in last 4 epochs\n",
      "Epoch [12/200], Batch [200/200], Train Loss: 0.5684, CONSISTENCY: 0.0006, RANKING: 0.5678\n",
      " CI: 0.7474735714190343  AUC: 0.754715061386861\n",
      "\n",
      " Val Metric: 1.502188632805895  Last Epoch took 178.60976767539978 seconds\n",
      "\n",
      " Validation metric has not improved in last 5 epochs\n",
      "Epoch [13/200], Batch [200/200], Train Loss: 0.5714, CONSISTENCY: 0.0011, RANKING: 0.5703\n",
      " CI: 0.7063525024285545  AUC: 0.7081086900664669\n",
      "\n",
      " Val Metric: 1.4144611924950214  Last Epoch took 178.90914273262024 seconds\n",
      "\n",
      " Validation metric has not improved in last 6 epochs\n",
      "Epoch [14/200], Batch [200/200], Train Loss: 0.5631, CONSISTENCY: 0.0010, RANKING: 0.5621\n",
      " CI: 0.68461873625004  AUC: 0.6735813083680317\n",
      "\n",
      " Val Metric: 1.3582000446180715  Last Epoch took 176.98852157592773 seconds\n",
      "\n",
      " Validation metric has not improved in last 7 epochs\n",
      "Epoch [15/200], Batch [200/200], Train Loss: 0.5703, CONSISTENCY: 0.0005, RANKING: 0.5698\n",
      " CI: 0.6825552480137719  AUC: 0.6755727991519026\n",
      "\n",
      " Val Metric: 1.3581280471656745  Last Epoch took 178.0335464477539 seconds\n",
      "\n",
      " Validation metric has not improved in last 8 epochs\n",
      "Epoch [16/200], Batch [200/200], Train Loss: 0.5638, CONSISTENCY: 0.0005, RANKING: 0.5633\n",
      " CI: 0.721187397656308  AUC: 0.7238972450750233\n",
      "\n",
      " Val Metric: 1.4450846427313313  Last Epoch took 180.08556580543518 seconds\n",
      "\n",
      " Validation metric has not improved in last 9 epochs\n",
      "Epoch [17/200], Batch [200/200], Train Loss: 0.5568, CONSISTENCY: 0.0014, RANKING: 0.5554\n",
      " CI: 0.7309837739825349  AUC: 0.7321741147523049\n",
      "\n",
      " Val Metric: 1.4631578887348398  Last Epoch took 178.0105106830597 seconds\n",
      "\n",
      " Validation metric has not improved in last 10 epochs\n",
      "Epoch [18/200], Batch [60/200], Train Loss: 0.5753, CONSISTENCY: 0.0001, RANKING: 0.5752\r"
     ]
    }
   ],
   "source": [
    "fld=5\n",
    "############### Data Loader #######################################\n",
    "\n",
    "val_data=validation_dataset()\n",
    "val_loader=DataLoader(dataset=val_data, batch_size=16, shuffle=False, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "\n",
    "train_data=train_dataset(fold=fld, prcnt=25, discard_converted=False)\n",
    "train_loader=DataLoader(dataset=train_data, batch_size=1, shuffle=True, num_workers=2, \n",
    "                             pin_memory=False, drop_last=False, worker_init_fn=worker_init_fn)\n",
    "\n",
    "################## Prepare the model  ############################### \n",
    "encoder_model=Encoder_Network2D()\n",
    "classifier_model=Classification_Network()\n",
    "\n",
    "\n",
    "encoder_model.to(device)\n",
    "classifier_model.to(device)\n",
    "\n",
    "############ Load source domain weights ##################\n",
    "# Load weights\n",
    "wt_nm='src_weight_fld5_metric1.4942585858396749.pt'\n",
    "\n",
    "checkpoint=torch.load(wt_nm)\n",
    "encoder_model.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "classifier_model.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "del checkpoint\n",
    "\n",
    "#######################################################################\n",
    "train_complete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
